---
title: "Nutzung von GeoDaten in den Sozialwissenschaften - Datenaufbereitung"
author: "Jan-Philipp Kolb"
date: "08 April 2016"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,echo=F}
Ex4=F
Ex1 <- T
```


## Die Daten editieren

```{r,eval=F,echo=F}
setwd("D:/Eigene Dateien/Dokumente/GitHub/GeoData/2016/slides/")
```


```{r,eval=Ex1}
load("data/refugeeTab.RData")
```


```{r,eval=Ex1}
mean(refugeeTab[,2])
```

um dies zu ändern ist ein wenig Kosmetik notwendig:

```{r,eval=Ex1}
refugeeTab[,2] <- as.numeric(refugeeTab[,2])
```


```{r,eval=Ex1}
mean(refugeeTab[,2])
```


## Die weiteren Spalten bearbeiten

- In R wird der Punkt als Dezimaltrennzeichen verwendet. 
- Wenn ein Komma im Ausdruck ist, wird der Eintrag als `character` behandelt
- dann kann bspw. kein Mittelwert berechnet werden

```{r,eval=Ex1}
refugeeTab[,3] <- gsub(",",".",refugeeTab[,3])
refugeeTab[,3] <- as.numeric(refugeeTab[,3])
```

## Erste Spalte bearbeiten und Daten speichern

```{r,eval=Ex1}
ab <- as.character(refugeeTab[,1])
info <- round(nchar(ab)/2)
Namen <- substr(ab,1,info)
Namen[1:29] <- gsub(" ","",Namen[1:29])
Namen[31]  <- "Zypern"
refugeeTab[,1] <- Namen
```

Spaltennamen verändern

```{r,eval=Ex1}
colnames(refugeeTab) <- c("Land","2015",
                          "pro_tsd_Einwohner")

```

Die Daten abspeichern

```{r,eval=F,echo=F}
setwd("D:/Eigene Dateien/Dokumente/GitHub/GeoData/2016/slides/data")
```

```{r,eval=F}
save(refugeeTab,file="refugeeTab_final.RData")
```


## Das Ergebnis

```{r,echo=F,eval=Ex1,warning=F}
library(knitr)
kable(head(refugeeTab))
```

Das Editing ist also aufwändiger als das eigentliche Scraping


## CO2 Verbrauch

```{r,eval=Ex4,echo=F}
link <- "https://en.wikipedia.org/wiki/List_of_countries_by_carbon_dioxide_emissions_per_capita"
```


```{r,eval=F}
link <- "https://en.wikipedia.org/wiki/
List_of_countries_by_carbon_dioxide_
emissions_per_capita"
```


```{r,cache=T,eval=Ex4}
link_data <- read_html(link)
doc <- htmlParse(link_data)
tab <- readHTMLTable(doc)
```

```{r,eval=F,echO=F}
save(tab,file="co2tab.RData")
```

```{r,echo=F}
load("data/co2tab.RData")
```

```{r}
str(tab)
```

## Die Elemente des Objektes

```{r}
tab[[1]]
```

```{r}
head(tab[[2]][,1:7])
```


## Auf die Daten schauen

```{r}
co2<- tab[[2]]
Cnames <- c("Rank","Country",paste("j",1990:2011,sep=""))
colnames(co2) <- Cnames 
```

## Haben die Daten die richtige Struktur?

```{r,warning=F}
mean(co2[,3])
```

```{r,warning=F}
co2[,3] <- as.numeric(as.character(co2[,3]))
```

```{r,warning=F}
for (i in 3:ncol(co2)){
  co2[,i] <- as.numeric(as.character(co2[,i]))
}
```

Daten speichern

```{r,eval=F,echo=F}
setwd("D:/Eigene Dateien/Dokumente/GitHub/GeoData/2016/slides/data")
```

```{r,eval=F}
save(co2,file="CO2emissions.RData")
```

## Eine Graphik

```{r,warning=F}
library(lattice)
emissions <- as.numeric(co2[,3])
names(emissions) <- co2[,2]
barchart(emissions[1:10],col="royalblue")
```


## Take Home Message

- Mit Webscraping können sehr viele Daten gewonnen werden.
- Allerdings kann die Datenaufbereitung sehr aufwändig sein. 
- Oftmals ist viel rumprobieren notwendig.